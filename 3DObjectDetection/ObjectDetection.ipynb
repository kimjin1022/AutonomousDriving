{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d14a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KITTI_ROOT : /home/jinjinjara1022/AutonomousDriving/datasets/kitti_object\n",
      "TRAIN_DIR  : /home/jinjinjara1022/AutonomousDriving/datasets/kitti_object/training\n",
      "TEST_DIR   : /home/jinjinjara1022/AutonomousDriving/datasets/kitti_object/testing (exists: True )\n"
     ]
    }
   ],
   "source": [
    "# 노트북 공통 설정\n",
    "import os, sys, pathlib, random, math, json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "KITTI_ROOT = \"/home/jinjinjara1022/AutonomousDriving/datasets/kitti_object\"\n",
    "TRAIN_DIR  = f\"{KITTI_ROOT}/training\"\n",
    "TEST_DIR   = f\"{KITTI_ROOT}/testing\"   # 없으면 자동으로 건너뜀\n",
    "\n",
    "print(\"KITTI_ROOT :\", KITTI_ROOT)\n",
    "print(\"TRAIN_DIR  :\", TRAIN_DIR)\n",
    "print(\"TEST_DIR   :\", TEST_DIR, \"(exists:\", os.path.isdir(TEST_DIR), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b2cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"training/image_2\": 7481,\n",
      "  \"training/label_2\": 7481,\n",
      "  \"training/calib\": 7481,\n",
      "  \"testing/image_2\": 7518,\n",
      "  \"testing/calib\": 7518\n",
      "}\n",
      "※ 기대값: train 7481개, test 7518개 (image_2/calib 기준)\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "def safe_count(p):\n",
    "    return len(glob.glob(os.path.join(p, \"*\")))\n",
    "\n",
    "counts = {\n",
    "    \"training/image_2\": safe_count(f\"{TRAIN_DIR}/image_2\"),\n",
    "    \"training/label_2\": safe_count(f\"{TRAIN_DIR}/label_2\"),\n",
    "    \"training/calib\":   safe_count(f\"{TRAIN_DIR}/calib\"),\n",
    "}\n",
    "\n",
    "if os.path.isdir(TEST_DIR):\n",
    "    counts.update({\n",
    "        \"testing/image_2\": safe_count(f\"{TEST_DIR}/image_2\"),\n",
    "        \"testing/calib\":   safe_count(f\"{TEST_DIR}/calib\"),\n",
    "    })\n",
    "\n",
    "print(json.dumps(counts, indent=2, ensure_ascii=False))\n",
    "print(\"※ 기대값: train 7481개, test 7518개 (image_2/calib 기준)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105d9309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] ImageSets 생성 완료: train=5984, val=1497 (test=7518)\n",
      "경로: /home/jinjinjara1022/AutonomousDriving/datasets/kitti_object/ImageSets\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "imagesets_dir = Path(KITTI_ROOT) / \"ImageSets\"\n",
    "imagesets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_imgs = sorted([p.stem for p in (Path(TRAIN_DIR)/\"image_2\").glob(\"*.png\")])\n",
    "assert len(train_imgs) > 0, \"training/image_2 비어있음\"\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_imgs)\n",
    "split = int(len(train_imgs)*0.8)\n",
    "\n",
    "train_ids = sorted(train_imgs[:split])\n",
    "val_ids   = sorted(train_imgs[split:])\n",
    "\n",
    "(Path(imagesets_dir/\"train.txt\")).write_text(\"\\n\".join(train_ids) + \"\\n\")\n",
    "(Path(imagesets_dir/\"val.txt\")).write_text(\"\\n\".join(val_ids) + \"\\n\")\n",
    "\n",
    "if os.path.isdir(TEST_DIR):\n",
    "    test_ids = sorted([p.stem for p in (Path(TEST_DIR)/\"image_2\").glob(\"*.png\")])\n",
    "    (Path(imagesets_dir/\"test.txt\")).write_text(\"\\n\".join(test_ids) + \"\\n\")\n",
    "\n",
    "print(f\"[✓] ImageSets 생성 완료: train={len(train_ids)}, val={len(val_ids)}\",\n",
    "      f\"(test={len(test_ids) if os.path.isdir(TEST_DIR) else 'N/A'})\")\n",
    "print(\"경로:\", imagesets_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdeed854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 라벨: 000000.txt objects: 1\n",
      "KittiObject(cls='Pedestrian', trunc=0.0, occ=0, alpha=-0.2, bbox=[712.4, 143.0, 810.73, 307.92], dims=[1.89, 0.48, 1.2], loc=[1.84, 1.47, 8.41], ry=0.01)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class KittiObject:\n",
    "    cls: str\n",
    "    trunc: float\n",
    "    occ: int\n",
    "    alpha: float\n",
    "    bbox: List[float]      # [l,t,r,b]\n",
    "    dims: List[float]      # [h,w,l]\n",
    "    loc:  List[float]      # [x,y,z] (camera coord)\n",
    "    ry:   float            # rotation_y (rad)\n",
    "\n",
    "def parse_label_file(path:str)->List[KittiObject]:\n",
    "    objs=[]\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            parts=line.split()\n",
    "            cls=parts[0]\n",
    "            if cls==\"DontCare\":  # 시각화/학습 혼동 방지 위해 제외\n",
    "                continue\n",
    "            trunc=float(parts[1]); occ=int(parts[2]); alpha=float(parts[3])\n",
    "            l,t,r,b=map(float, parts[4:8])\n",
    "            h,w,l3=map(float, parts[8:11])\n",
    "            x,y,z=map(float, parts[11:14])\n",
    "            ry=float(parts[14])\n",
    "            objs.append(KittiObject(cls,trunc,occ,alpha,[l,t,r,b],[h,w,l3],[x,y,z],ry))\n",
    "    return objs\n",
    "\n",
    "# 샘플 확인\n",
    "sample_label = sorted((Path(TRAIN_DIR)/\"label_2\").glob(\"*.txt\"))[0]\n",
    "parsed = parse_label_file(str(sample_label))\n",
    "print(\"샘플 라벨:\", sample_label.name, \"objects:\", len(parsed))\n",
    "if parsed:\n",
    "    print(parsed[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "649124d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] 2D 미리보기 저장: /home/jinjinjara1022/AutonomousDriving/datasets/kitti_object/preview_2d\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "out_dir = Path(KITTI_ROOT)/\"preview_2d\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "all_ids = [p.stem for p in (Path(TRAIN_DIR)/\"image_2\").glob(\"*.png\")]\n",
    "random.seed(0)\n",
    "sample_ids = random.sample(all_ids, min(10, len(all_ids)))\n",
    "\n",
    "for sid in sample_ids:\n",
    "    img_path = Path(TRAIN_DIR)/\"image_2\"/f\"{sid}.png\"\n",
    "    lbl_path = Path(TRAIN_DIR)/\"label_2\"/f\"{sid}.txt\"\n",
    "    if not lbl_path.exists():\n",
    "        continue\n",
    "    img = cv2.imread(str(img_path))\n",
    "    for obj in parse_label_file(str(lbl_path)):\n",
    "        l,t,r,b = map(int, obj.bbox)\n",
    "        cv2.rectangle(img,(l,t),(r,b),(0,255,0),2)\n",
    "        cv2.putText(img, obj.cls, (l,max(15,t-5)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "    cv2.imwrite(str(out_dir/f\"{sid}.jpg\"), img)\n",
    "\n",
    "print(\"[✓] 2D 미리보기 저장:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee37834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['P0', 'P1', 'P2', 'P3', 'R0_rect', 'Tr_velo_to_cam', 'Tr_imu_to_velo'])\n",
      "P2:\n",
      " [[ 7.070493e+02  0.000000e+00  6.040814e+02  4.575831e+01]\n",
      " [ 0.000000e+00  7.070493e+02  1.805066e+02 -3.454157e-01]\n",
      " [ 0.000000e+00  0.000000e+00  1.000000e+00  4.981016e-03]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_calib_file(path:str)->Dict[str, np.ndarray]:\n",
    "    data={}\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line: \n",
    "                continue\n",
    "            k, v = line.strip().split(\":\", 1)\n",
    "            v = v.strip()\n",
    "            nums = list(map(float, v.split()))\n",
    "            if k.startswith(\"P\") and len(nums)==12:\n",
    "                data[k] = np.array(nums, dtype=np.float32).reshape(3,4)\n",
    "            elif k in (\"R0_rect\", \"R_rect\") and len(nums)==9:\n",
    "                data[k] = np.array(nums, dtype=np.float32).reshape(3,3)\n",
    "            elif k==\"Tr_velo_to_cam\" and len(nums)==12:\n",
    "                data[k] = np.array(nums, dtype=np.float32).reshape(3,4)\n",
    "            else:\n",
    "                # 기타 항목은 필요 시 추가\n",
    "                try:\n",
    "                    data[k] = np.array(nums, dtype=np.float32)\n",
    "                except:\n",
    "                    data[k] = v\n",
    "    return data\n",
    "\n",
    "# 샘플 확인\n",
    "sample_calib = sorted((Path(TRAIN_DIR)/\"calib\").glob(\"*.txt\"))[0]\n",
    "C = read_calib_file(str(sample_calib))\n",
    "print(\"keys:\", C.keys())\n",
    "print(\"P2:\\n\", C.get(\"P2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984fd36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] 3D 박스 미리보기 저장: /home/jinjinjara1022/AutonomousDriving/datasets/kitti_object/preview_3d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, cv2\n",
    "from pathlib import Path\n",
    "import random, math\n",
    "\n",
    "def compute_box_3d(dim, loc, ry):\n",
    "    \"\"\"\n",
    "    dim = [h, w, l]\n",
    "    loc = [x, y, z] (camera coord)\n",
    "    ry: rotation around Y-axis in camera coordinates\n",
    "    반환: (3,8) 카메라 좌표계의 8개 코너 (x,y,z)\n",
    "    \"\"\"\n",
    "    h, w, l = dim\n",
    "    # object 좌표계 기준 코너 (KITTI 관례: 바닥중심이 loc)\n",
    "    x_corners = [ l/2,  l/2, -l/2, -l/2,  l/2,  l/2, -l/2, -l/2]\n",
    "    y_corners = [   0,    0,    0,    0,  -h,  -h,   -h,   -h]\n",
    "    z_corners = [ w/2, -w/2, -w/2,  w/2,  w/2, -w/2, -w/2,  w/2]\n",
    "    corners = np.vstack([x_corners, y_corners, z_corners])  # (3,8)\n",
    "\n",
    "    # 회전 (Y축)\n",
    "    R = np.array([[ math.cos(ry), 0, math.sin(ry)],\n",
    "                  [ 0,            1, 0           ],\n",
    "                  [-math.sin(ry), 0, math.cos(ry)]], dtype=np.float32)\n",
    "    corners_rot = R @ corners\n",
    "\n",
    "    # 이동\n",
    "    corners_3d = corners_rot + np.array(loc, dtype=np.float32).reshape(3,1)\n",
    "    return corners_3d\n",
    "\n",
    "def project_to_image(pts_3d, P):\n",
    "    \"\"\"\n",
    "    pts_3d: (3, N)\n",
    "    P: (3,4) projection matrix\n",
    "    return: (2,N) image coords\n",
    "    \"\"\"\n",
    "    n = pts_3d.shape[1]\n",
    "    homo = np.vstack([pts_3d, np.ones((1,n), dtype=np.float32)])  # (4,N)\n",
    "    pts_2d_homo = P @ homo  # (3,N)\n",
    "    pts_2d = pts_2d_homo[:2] / np.clip(pts_2d_homo[2:], 1e-6, None)\n",
    "    return pts_2d\n",
    "\n",
    "def draw_projected_box3d(img, qs, color=(0,0,255), thickness=2):\n",
    "    \"\"\"\n",
    "    qs: (2,8) projected corner points\n",
    "    코너 인덱스 연결 규칙에 따라 12개 에지 그리기\n",
    "    \"\"\"\n",
    "    qs = qs.T.astype(int)  # (8,2)\n",
    "    # 연결 (윗면/아랫면/수직 에지)\n",
    "    edges = [(0,1),(1,2),(2,3),(3,0),\n",
    "             (4,5),(5,6),(6,7),(7,4),\n",
    "             (0,4),(1,5),(2,6),(3,7)]\n",
    "    for i,j in edges:\n",
    "        cv2.line(img, tuple(qs[i]), tuple(qs[j]), color, thickness)\n",
    "    return img\n",
    "\n",
    "# 샘플 몇 장 그려보기 (Car/Pedestrian/Cyclist 제한)\n",
    "out_dir_3d = Path(KITTI_ROOT)/\"preview_3d\"\n",
    "out_dir_3d.mkdir(exist_ok=True)\n",
    "\n",
    "ids = [p.stem for p in (Path(TRAIN_DIR)/\"image_2\").glob(\"*.png\")]\n",
    "random.seed(1)\n",
    "for sid in random.sample(ids, min(10, len(ids))):\n",
    "    img_path = Path(TRAIN_DIR)/\"image_2\"/f\"{sid}.png\"\n",
    "    lbl_path = Path(TRAIN_DIR)/\"label_2\"/f\"{sid}.txt\"\n",
    "    calib_path = Path(TRAIN_DIR)/\"calib\"/f\"{sid}.txt\"\n",
    "    if not (lbl_path.exists() and calib_path.exists()):\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(str(img_path))\n",
    "    cal = read_calib_file(str(calib_path))\n",
    "    P2  = cal[\"P2\"]\n",
    "\n",
    "    objects = [o for o in parse_label_file(str(lbl_path))\n",
    "               if o.cls in (\"Car\",\"Pedestrian\",\"Cyclist\")]\n",
    "\n",
    "    for o in objects:\n",
    "        # 2D 박스(초록) + 3D 박스(빨강)\n",
    "        l,t,r,b = map(int, o.bbox)\n",
    "        cv2.rectangle(img,(l,t),(r,b),(0,255,0),2)\n",
    "\n",
    "        # 3D box\n",
    "        corners_3d = compute_box_3d(o.dims, o.loc, o.ry)\n",
    "        # z<=0 포인트(카메라 뒤)는 skip\n",
    "        if (corners_3d[2] <= 0).any():\n",
    "            continue\n",
    "        pts_2d = project_to_image(corners_3d, P2)  # (2,8)\n",
    "        img = draw_projected_box3d(img, pts_2d, (0,0,255), 2)\n",
    "\n",
    "        # 클래스 라벨\n",
    "        cv2.putText(img, o.cls, (l,max(15,t-5)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imwrite(str(out_dir_3d/f\"{sid}.jpg\"), img)\n",
    "\n",
    "print(\"[✓] 3D 박스 미리보기 저장:\", out_dir_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "203cea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 5984 val len: 1497\n",
      "image: torch.Size([3, 370, 1224]) num objs: 1 P2 shape: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class KittiMono3DDataset(Dataset):\n",
    "    def __init__(self, kitti_root:str, split:str=\"train\"):\n",
    "        self.kitti_root = Path(kitti_root)\n",
    "        self.split = split\n",
    "        ids_path = self.kitti_root/\"ImageSets\"/f\"{split}.txt\"\n",
    "        assert ids_path.exists(), f\"{ids_path} 없음\"\n",
    "        self.ids = ids_path.read_text().strip().splitlines()\n",
    "        self.img_dir = self.kitti_root/\"training\"/\"image_2\"\n",
    "        self.lbl_dir = self.kitti_root/\"training\"/\"label_2\"\n",
    "        self.calib_dir = self.kitti_root/\"training\"/\"calib\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.ids[idx]\n",
    "        img = cv2.imread(str(self.img_dir/f\"{sid}.png\"))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        labels = parse_label_file(str(self.lbl_dir/f\"{sid}.txt\"))\n",
    "        calib  = read_calib_file(str(self.calib_dir/f\"{sid}.txt\"))\n",
    "        P2     = calib[\"P2\"]\n",
    "\n",
    "        # 간단한 텐서 변환 (실제 학습에 맞게 Normalize/Resize 등 추가 예정)\n",
    "        img_t = torch.from_numpy(img).permute(2,0,1).float()  # (3,H,W)\n",
    "        meta = {\"id\": sid, \"H\": h, \"W\": w, \"P2\": P2}\n",
    "\n",
    "        # 라벨을 텐서로 간이 변환 (학습용 포맷은 이후 모델 설계에 맞춰 변환)\n",
    "        target = {\n",
    "            \"classes\": [o.cls for o in labels],\n",
    "            \"bbox2d\":  np.array([o.bbox for o in labels], dtype=np.float32),  # (N,4)\n",
    "            \"dims\":    np.array([o.dims for o in labels], dtype=np.float32),  # (N,3)\n",
    "            \"loc\":     np.array([o.loc  for o in labels], dtype=np.float32),  # (N,3)\n",
    "            \"ry\":      np.array([o.ry   for o in labels], dtype=np.float32),  # (N,)\n",
    "        }\n",
    "        return img_t, target, meta\n",
    "\n",
    "# 데이터셋 빠른 점검\n",
    "train_ds = KittiMono3DDataset(KITTI_ROOT, \"train\")\n",
    "val_ds   = KittiMono3DDataset(KITTI_ROOT, \"val\")\n",
    "print(\"train len:\", len(train_ds), \"val len:\", len(val_ds))\n",
    "\n",
    "img_t, target, meta = train_ds[0]\n",
    "print(\"image:\", img_t.shape, \"num objs:\", len(target[\"classes\"]), \"P2 shape:\", meta[\"P2\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6f713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] id=003238  img=(3, 375, 1242)  objs=7\n",
      "[1] id=006762  img=(3, 375, 1242)  objs=4\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=lambda x: x)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "for i,(img_t, tgt, meta) in enumerate(batch):\n",
    "    print(f\"[{i}] id={meta['id']}  img={tuple(img_t.shape)}  objs={len(tgt['classes'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c51c2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Car\": {\n",
      "    \"mean\": [\n",
      "      1.5260810852050781,\n",
      "      1.6286139488220215,\n",
      "      3.8839685916900635\n",
      "    ],\n",
      "    \"std\": [\n",
      "      0.1366989016532898,\n",
      "      0.10216215252876282,\n",
      "      0.42591333389282227\n",
      "    ],\n",
      "    \"count\": 28742\n",
      "  },\n",
      "  \"Pedestrian\": {\n",
      "    \"mean\": [\n",
      "      1.7607048749923706,\n",
      "      0.6601871848106384,\n",
      "      0.8422839045524597\n",
      "    ],\n",
      "    \"std\": [\n",
      "      0.11325047165155411,\n",
      "      0.14265143871307373,\n",
      "      0.23489883542060852\n",
      "    ],\n",
      "    \"count\": 4487\n",
      "  },\n",
      "  \"Cyclist\": {\n",
      "    \"mean\": [\n",
      "      1.7372057437896729,\n",
      "      0.5967734456062317,\n",
      "      1.7635438442230225\n",
      "    ],\n",
      "    \"std\": [\n",
      "      0.09479568153619766,\n",
      "      0.12417350709438324,\n",
      "      0.17660865187644958\n",
      "    ],\n",
      "    \"count\": 1627\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5260811 , 1.628614  , 3.8839686 ],\n",
       "       [1.7607049 , 0.6601872 , 0.8422839 ],\n",
       "       [1.7372057 , 0.59677345, 1.7635438 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json, math\n",
    "\n",
    "CLASSES = [\"Car\", \"Pedestrian\", \"Cyclist\"]\n",
    "\n",
    "def collect_class_stats(label_dir):\n",
    "    stats = {c: [] for c in CLASSES}\n",
    "    for p in sorted(Path(label_dir).glob(\"*.txt\")):\n",
    "        for o in parse_label_file(str(p)):\n",
    "            if o.cls in CLASSES:\n",
    "                stats[o.cls].append(o.dims)  # [h,w,l]\n",
    "    out = {}\n",
    "    for c, arr in stats.items():\n",
    "        if len(arr)==0:\n",
    "            out[c] = {\"mean\":[1.0,1.0,1.0], \"count\":0}\n",
    "            continue\n",
    "        a = np.array(arr, dtype=np.float32)\n",
    "        out[c] = {\"mean\": a.mean(0).tolist(), \"std\": a.std(0).tolist(), \"count\": int(a.shape[0])}\n",
    "    return out\n",
    "\n",
    "priors = collect_class_stats(f\"{TRAIN_DIR}/label_2\")\n",
    "print(json.dumps(priors, indent=2))\n",
    "class_to_idx = {c:i for i,c in enumerate(CLASSES)}\n",
    "priors_arr = np.stack([np.array(priors[c][\"mean\"], dtype=np.float32) for c in CLASSES], 0)  # (3,3)\n",
    "priors_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a42e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] objects: 27974\n",
      "[val] objects: 6882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'img': torch.Size([3, 128, 128]),\n",
       " 'cls_idx': torch.Size([]),\n",
       " 'prior': torch.Size([3]),\n",
       " 'dims_res': torch.Size([3]),\n",
       " 'logz': torch.Size([]),\n",
       " 'yaw': torch.Size([2]),\n",
       " 'uv': torch.Size([2]),\n",
       " 'P2': torch.Size([3, 4])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2, numpy as np\n",
    "\n",
    "IMG_SIZE = 128\n",
    "PADDING_SCALE = 1.2  # bbox에 여유\n",
    "\n",
    "def crop_resize(img, bbox, out_size=IMG_SIZE, scale=PADDING_SCALE):\n",
    "    h, w = img.shape[:2]\n",
    "    l,t,r,b = bbox\n",
    "    cx, cy = (l+r)/2, (t+b)/2\n",
    "    bw, bh = (r-l), (b-t)\n",
    "    s = max(bw, bh) * scale\n",
    "    x1, y1 = int(cx - s/2), int(cy - s/2)\n",
    "    x2, y2 = int(cx + s/2), int(cy + s/2)\n",
    "    # pad if out of bounds\n",
    "    pad_l = max(0, -x1); pad_t = max(0, -y1)\n",
    "    pad_r = max(0, x2 - w); pad_b = max(0, y2 - h)\n",
    "    if any([pad_l,pad_t,pad_r,pad_b]):\n",
    "        img = cv2.copyMakeBorder(img, pad_t, pad_b, pad_l, pad_r, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "        x1 += pad_l; x2 += pad_l; y1 += pad_t; y2 += pad_t\n",
    "    crop = img[y1:y2, x1:x2]\n",
    "    crop = cv2.resize(crop, (out_size, out_size), interpolation=cv2.INTER_LINEAR)\n",
    "    return crop\n",
    "\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "class KittiObjectROIs(Dataset):\n",
    "    def __init__(self, kitti_root:str, split:str=\"train\", classes=CLASSES, max_samples=None):\n",
    "        self.root = Path(kitti_root)\n",
    "        self.split = split\n",
    "        self.classes = set(classes)\n",
    "        ids = (self.root/\"ImageSets\"/f\"{split}.txt\").read_text().strip().splitlines()\n",
    "        self.img_dir = self.root/\"training\"/\"image_2\"\n",
    "        self.lbl_dir = self.root/\"training\"/\"label_2\"\n",
    "        self.calib_dir = self.root/\"training\"/\"calib\"\n",
    "        self.items = []\n",
    "        for sid in ids:\n",
    "            lbl_path = self.lbl_dir/f\"{sid}.txt\"\n",
    "            if not lbl_path.exists(): \n",
    "                continue\n",
    "            for o in parse_label_file(str(lbl_path)):\n",
    "                if o.cls in self.classes:\n",
    "                    self.items.append((sid, o))\n",
    "        if max_samples:\n",
    "            self.items = self.items[:max_samples]\n",
    "        print(f\"[{split}] objects: {len(self.items)}\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid, o = self.items[idx]\n",
    "        img_bgr = cv2.imread(str(self.img_dir/f\"{sid}.png\"))\n",
    "        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        crop = crop_resize(img, o.bbox, IMG_SIZE, PADDING_SCALE)\n",
    "        # to tensor (ImageNet norm)\n",
    "        x = (crop.astype(np.float32)/255.0 - IMAGENET_MEAN)/IMAGENET_STD\n",
    "        x = torch.from_numpy(x).permute(2,0,1)  # (3,128,128)\n",
    "\n",
    "        cls_idx = class_to_idx[o.cls]\n",
    "        prior = torch.from_numpy(priors_arr[cls_idx])  # (3,)\n",
    "        dims = torch.tensor(o.dims, dtype=torch.float32)\n",
    "        dims_res = dims - prior\n",
    "\n",
    "        z = torch.tensor(o.loc[2], dtype=torch.float32)\n",
    "        logz = torch.log(torch.clamp(z, min=1e-3))\n",
    "\n",
    "        ry = torch.tensor(o.ry, dtype=torch.float32)\n",
    "        yaw_tgt = torch.stack([torch.sin(ry), torch.cos(ry)], 0)  # (2,)\n",
    "\n",
    "        # 2D center (u,v)\n",
    "        l,t,r,b = o.bbox\n",
    "        uv = torch.tensor([(l+r)/2.0, (t+b)/2.0], dtype=torch.float32)\n",
    "\n",
    "        # calib\n",
    "        P2 = torch.from_numpy(read_calib_file(str(self.calib_dir/f\"{sid}.txt\"))[\"P2\"])\n",
    "\n",
    "        return {\n",
    "            \"img\": x,\n",
    "            \"cls_idx\": torch.tensor(cls_idx, dtype=torch.long),\n",
    "            \"prior\": prior,\n",
    "            \"dims_res\": dims_res,\n",
    "            \"logz\": logz,\n",
    "            \"yaw\": yaw_tgt,\n",
    "            \"uv\": uv,\n",
    "            \"P2\": P2,\n",
    "            \"sid\": sid\n",
    "        }\n",
    "\n",
    "# 샘플 확인\n",
    "train_roi = KittiObjectROIs(KITTI_ROOT, \"train\", max_samples=None)\n",
    "val_roi   = KittiObjectROIs(KITTI_ROOT, \"val\", max_samples=None)\n",
    "\n",
    "sample = train_roi[0]\n",
    "{ k: (v.shape if hasattr(v, 'shape') else v) for k,v in sample.items() if k not in ('sid',) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f6321fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.34151"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Mono3DHead(nn.Module):\n",
    "    def __init__(self, feat_dim=512, out_dim=6):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "    def forward(self, f):\n",
    "        return self.fc(f)  # (B,6)\n",
    "\n",
    "class Mono3DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        # 입력 3x128x128 → resnet18 avgpool → 512\n",
    "        self.backbone = nn.Sequential(*(list(m.children())[:-1]))  # drop fc\n",
    "        self.head = Mono3DHead(512, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)         # (B, 512, 1, 1)\n",
    "        f = torch.flatten(f, 1)      # (B, 512)\n",
    "        out = self.head(f)           # (B, 6)\n",
    "        dims_res = out[:, 0:3]\n",
    "        logz     = out[:, 3:4]\n",
    "        yaw_raw  = out[:, 4:6]       # not normalized yet\n",
    "        # normalize yaw vector\n",
    "        yaw_norm = torch.linalg.norm(yaw_raw, dim=1, keepdim=True) + 1e-6\n",
    "        yaw = yaw_raw / yaw_norm\n",
    "        return dims_res, logz, yaw  # yaw: (sin,cos) normalized\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Mono3DNet().to(device)\n",
    "sum(p.numel() for p in model.parameters())/1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a412b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 각 샘플은 dict. 키별로 스택\n",
    "    out = {}\n",
    "    for k in batch[0].keys():\n",
    "        if isinstance(batch[0][k], torch.Tensor):\n",
    "            out[k] = torch.stack([b[k] for b in batch], 0)\n",
    "        else:\n",
    "            out[k] = [b[k] for b in batch]\n",
    "    return out\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_roi, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_roi,   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# 손실 가중치\n",
    "W_DIM = 1.0\n",
    "W_Z   = 1.0\n",
    "W_YAW = 1.0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "def loss_fn(batch, pred):\n",
    "    dims_res_pred, logz_pred, yaw_pred = pred   # yaw_pred normalized\n",
    "\n",
    "    dims_res_tgt = batch[\"dims_res\"].to(device)     # (B,3)\n",
    "    prior        = batch[\"prior\"].to(device)        # (B,3)\n",
    "    dims_pred    = dims_res_pred + prior            # 최종 dims\n",
    "    dims_tgt     = dims_res_tgt + prior             # = batch dims\n",
    "\n",
    "    # L1 on dims\n",
    "    loss_dims = (dims_pred - dims_tgt).abs().mean()\n",
    "\n",
    "    # depth on logZ (L1)\n",
    "    loss_z = (logz_pred.squeeze(1) - batch[\"logz\"].to(device)).abs().mean()\n",
    "\n",
    "    # yaw pred vs target (MSE on normalized vector)\n",
    "    loss_yaw = ((yaw_pred - batch[\"yaw\"].to(device))**2).mean()\n",
    "\n",
    "    loss = W_DIM*loss_dims + W_Z*loss_z + W_YAW*loss_yaw\n",
    "    return loss, {\"loss_dims\": loss_dims.item(), \"loss_z\": loss_z.item(), \"loss_yaw\": loss_yaw.item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5f9e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] train 0.4841 (dims 0.138 z 0.161 yaw 0.185) | val 0.4125 (dims 0.132 z 0.121 yaw 0.159)  [102.1s]\n",
      "  -> saved: mono3d_baseline.pt\n",
      "[2/5] train 0.3889 (dims 0.123 z 0.153 yaw 0.112) | val 0.3873 (dims 0.121 z 0.146 yaw 0.121)  [146.7s]\n",
      "  -> saved: mono3d_baseline.pt\n",
      "[3/5] train 0.3422 (dims 0.112 z 0.139 yaw 0.090) | val 0.3378 (dims 0.114 z 0.109 yaw 0.114)  [150.9s]\n",
      "  -> saved: mono3d_baseline.pt\n",
      "[4/5] train 0.2956 (dims 0.103 z 0.125 yaw 0.067) | val 0.2968 (dims 0.105 z 0.105 yaw 0.087)  [134.4s]\n",
      "  -> saved: mono3d_baseline.pt\n",
      "[5/5] train 0.3307 (dims 0.103 z 0.134 yaw 0.094) | val 0.3100 (dims 0.106 z 0.103 yaw 0.101)  [143.8s]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total, logs = 0.0, {\"loss_dims\":0.0, \"loss_z\":0.0, \"loss_yaw\":0.0}\n",
    "    for batch in loader:\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}\n",
    "        pred = model(batch[\"img\"])\n",
    "        loss, d = loss_fn(batch, pred)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        for k in logs: logs[k]+= d[k]\n",
    "    n=len(loader)\n",
    "    return total/n, {k: v/n for k,v in logs.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total, logs = 0.0, {\"loss_dims\":0.0, \"loss_z\":0.0, \"loss_yaw\":0.0}\n",
    "    for batch in loader:\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}\n",
    "        pred = model(batch[\"img\"])\n",
    "        loss, d = loss_fn(batch, pred)\n",
    "        total += loss.item()\n",
    "        for k in logs: logs[k]+= d[k]\n",
    "    n=len(loader)\n",
    "    return total/n, {k: v/n for k,v in logs.items()}\n",
    "\n",
    "EPOCHS = 5\n",
    "best = 1e9\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    t0=time()\n",
    "    tr, tr_log = train_one_epoch(model, train_loader, optimizer)\n",
    "    va, va_log = eval_one_epoch(model, val_loader)\n",
    "    dt=time()-t0\n",
    "    print(f\"[{ep}/{EPOCHS}] train {tr:.4f} (dims {tr_log['loss_dims']:.3f} z {tr_log['loss_z']:.3f} yaw {tr_log['loss_yaw']:.3f}) | \"\n",
    "          f\"val {va:.4f} (dims {va_log['loss_dims']:.3f} z {va_log['loss_z']:.3f} yaw {va_log['loss_yaw']:.3f})  [{dt:.1f}s]\")\n",
    "    if va < best:\n",
    "        best = va\n",
    "        torch.save(model.state_dict(), \"./models/mono3d_baseline.pt\")\n",
    "        print(\"  -> saved:\", \"mono3d_baseline.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27f29660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: preview_pred_fixed\n"
     ]
    }
   ],
   "source": [
    "# === 1) compute_box_3d 축 교정 (x=width, z=length) ===\n",
    "import numpy as np, cv2\n",
    "import torch\n",
    "from math import atan2\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_box_3d_fixed(dim, loc, ry):\n",
    "    \"\"\"\n",
    "    dim=[h,w,l], loc=[X,Y,Z] (camera coord), ry: yaw around Y\n",
    "    return: (3,8) camera coords\n",
    "    \"\"\"\n",
    "    h, w, l = dim\n",
    "    # x=±w/2, z=±l/2  (※ 기존 코드에서 w/l이 뒤바뀌어 있었음)\n",
    "    x_c = [ w/2,  w/2, -w/2, -w/2,  w/2,  w/2, -w/2, -w/2]\n",
    "    y_c = [   0,    0,    0,    0,  -h,  -h,   -h,   -h]\n",
    "    z_c = [ l/2, -l/2, -l/2,  l/2,  l/2, -l/2, -l/2,  l/2]\n",
    "    corners = np.vstack([x_c, y_c, z_c]).astype(np.float32)\n",
    "\n",
    "    c, s = np.cos(ry), np.sin(ry)\n",
    "    R = np.array([[ c, 0,  s],\n",
    "                  [ 0, 1,  0],\n",
    "                  [-s, 0,  c]], dtype=np.float32)\n",
    "    corners = R @ corners\n",
    "    corners = corners + np.array(loc, dtype=np.float32).reshape(3,1)\n",
    "    return corners\n",
    "\n",
    "# draw 함수는 그대로 사용 가능 (project_to_image, draw_projected_box3d 등)\n",
    "\n",
    "# === 2) 하단 중앙(uv_bottom) 사용 ===\n",
    "def uv_center_and_bottom_from_bbox(bbox):\n",
    "    l,t,r,b = bbox\n",
    "    u_c, v_c = (l+r)/2.0, (t+b)/2.0\n",
    "    u_b, v_b = (l+r)/2.0, b\n",
    "    return (u_c, v_c), (u_b, v_b)\n",
    "\n",
    "def uvz_to_xyz(uv, Z, P2):\n",
    "    fx, fy = P2[0,0], P2[1,1]\n",
    "    cx, cy = P2[0,2], P2[1,2]\n",
    "    u, v = uv\n",
    "    X = (u - cx) * Z / fx\n",
    "    Y = (v - cy) * Z / fy\n",
    "    return np.array([X, Y, Z], dtype=np.float32)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_on_samples_fixed(model, dataset, n=8, seed=0, use_bottom=True):\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.choice(len(dataset), size=min(n, len(dataset)), replace=False)\n",
    "    outs=[]\n",
    "    for i in idxs:\n",
    "        item = dataset[i]                        # dict (img, prior, …)\n",
    "        sid, gt_obj = dataset.items[i]          # ← 같은 dataset에서 GT 객체 접근\n",
    "        # 모델 추론\n",
    "        x = item[\"img\"].unsqueeze(0).to(device)\n",
    "        dims_res, logz, yaw = model(x)\n",
    "        prior = item[\"prior\"].cpu().numpy()\n",
    "        dims_pred = (dims_res.squeeze(0).cpu().numpy() + prior)    # [h,w,l]\n",
    "        Z = float(torch.exp(logz.squeeze()).cpu().numpy())\n",
    "        yaw_vec = yaw.squeeze(0).cpu().numpy()\n",
    "        ry = float(atan2(yaw_vec[0], yaw_vec[1]))\n",
    "\n",
    "        # uv 선택: 바닥 중앙 vs 중심\n",
    "        bbox = gt_obj.bbox\n",
    "        (u_c, v_c), (u_b, v_b) = uv_center_and_bottom_from_bbox(bbox)\n",
    "        uv = (u_b, v_b) if use_bottom else (u_c, v_c)\n",
    "\n",
    "        P2 = item[\"P2\"].cpu().numpy()\n",
    "        loc = uvz_to_xyz(uv, Z, P2)\n",
    "        outs.append((i, dims_pred, loc, ry))\n",
    "    return outs\n",
    "\n",
    "# === 3) 올바른 GT 2D 박스 참조 & 3D 투영 ===\n",
    "def draw_pred_boxes_fixed(dataset, preds, out_dir):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, dims, loc, ry in preds:\n",
    "        sid, gt_obj = dataset.items[i]                         # ← 같은 dataset에서\n",
    "        img_path = Path(TRAIN_DIR)/\"image_2\"/f\"{sid}.png\"\n",
    "        img = cv2.imread(str(img_path))\n",
    "        P2 = dataset[i][\"P2\"].cpu().numpy()                    # from item\n",
    "\n",
    "        # Pred 3D box\n",
    "        corners_3d = compute_box_3d_fixed(dims, loc, ry)\n",
    "        if (corners_3d[2] <= 0).any():\n",
    "            continue\n",
    "        pts_2d = project_to_image(corners_3d, P2)\n",
    "        img = draw_projected_box3d(img, pts_2d, (0,165,255), 2)  # 주황\n",
    "\n",
    "        # GT 2D box (초록)\n",
    "        l,t,r,b = map(int, gt_obj.bbox)\n",
    "        cv2.rectangle(img,(l,t),(r,b),(0,255,0),2)\n",
    "\n",
    "        cv2.imwrite(str(out_dir/f\"{sid}_pred.jpg\"), img)\n",
    "    return out_dir\n",
    "\n",
    "# 실행\n",
    "preds = predict_on_samples_fixed(model, val_roi, n=12, seed=1, use_bottom=True)\n",
    "out_dir = draw_pred_boxes_fixed(val_roi, preds, f\"./preview_pred_fixed\")\n",
    "print(\"saved to:\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d70558ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: preview_pred\n"
     ]
    }
   ],
   "source": [
    "# 프레임별로 예측을 모아서 한 장에 모두 그려 저장\n",
    "import cv2, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from math import atan2\n",
    "\n",
    "# compute_box_3d_fixed / project_to_image / draw_projected_box3d / read_calib_file\n",
    "# / uvz_to_xyz / uv_center_and_bottom_from_bbox 가 위 셀에 있다고 가정\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_grouped_by_sid(model, dataset, use_bottom=True, max_frames=20):\n",
    "    model.eval()\n",
    "    # 1) 밸리데이션에 등장하는 프레임 id 목록\n",
    "    sids = []\n",
    "    seen = set()\n",
    "    for sid, _ in dataset.items:\n",
    "        if sid not in seen:\n",
    "            seen.add(sid)\n",
    "            sids.append(sid)\n",
    "    sids = sids[:max_frames]\n",
    "\n",
    "    sid_to_preds = defaultdict(list)\n",
    "    for i,(sid, gt_obj) in enumerate(dataset.items):\n",
    "        if sid not in set(sids):\n",
    "            continue\n",
    "        item = dataset[i]  # dict\n",
    "        # 추론\n",
    "        x = item[\"img\"].unsqueeze(0).to(device)\n",
    "        dims_res, logz, yaw = model(x)\n",
    "        prior = item[\"prior\"].cpu().numpy()\n",
    "        dims_pred = (dims_res.squeeze(0).cpu().numpy() + prior)   # [h,w,l]\n",
    "        Z = float(torch.exp(logz.squeeze()).cpu().numpy())\n",
    "        yaw_vec = yaw.squeeze(0).cpu().numpy()\n",
    "        ry = float(atan2(yaw_vec[0], yaw_vec[1]))\n",
    "        # uv: 바닥 중앙 권장\n",
    "        (u_c, v_c), (u_b, v_b) = uv_center_and_bottom_from_bbox(gt_obj.bbox)\n",
    "        uv = (u_b, v_b) if use_bottom else (u_c, v_c)\n",
    "        P2 = item[\"P2\"].cpu().numpy()\n",
    "        loc = uvz_to_xyz(uv, Z, P2)\n",
    "        sid_to_preds[sid].append((dims_pred, loc, ry))\n",
    "    return sids, sid_to_preds\n",
    "\n",
    "def draw_grouped_results(dataset, sids, sid_to_preds, out_dir=\"./preview_pred_grouped\"):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for sid in sids:\n",
    "        img_path  = Path(TRAIN_DIR)/\"image_2\"/f\"{sid}.png\"\n",
    "        lbl_path  = Path(TRAIN_DIR)/\"label_2\"/f\"{sid}.txt\"\n",
    "        calib_path= Path(TRAIN_DIR)/\"calib\"/f\"{sid}.txt\"\n",
    "        img = cv2.imread(str(img_path))\n",
    "        P2  = read_calib_file(str(calib_path))[\"P2\"]\n",
    "\n",
    "        # GT 3D box (초록)\n",
    "        for gt in [o for o in parse_label_file(str(lbl_path)) if o.cls in (\"Car\",\"Pedestrian\",\"Cyclist\")]:\n",
    "            corners_3d = compute_box_3d_fixed(gt.dims, gt.loc, gt.ry)\n",
    "            if not (corners_3d[2] > 0).all(): \n",
    "                continue\n",
    "            pts_2d = project_to_image(corners_3d, P2)\n",
    "            img = draw_projected_box3d(img, pts_2d, (0,255,0), 2)\n",
    "\n",
    "        # Pred 3D boxes (주황), 프레임 내 모든 ROI 예측 그리기\n",
    "        for dims_pred, loc, ry in sid_to_preds.get(sid, []):\n",
    "            corners_3d = compute_box_3d_fixed(dims_pred, loc, ry)\n",
    "            if (corners_3d[2] <= 0).any():\n",
    "                continue\n",
    "            pts_2d = project_to_image(corners_3d, P2)\n",
    "            img = draw_projected_box3d(img, pts_2d, (0,165,255), 2)\n",
    "\n",
    "        cv2.imwrite(str(out_dir/f\"{sid}_all.jpg\"), img)\n",
    "    return out_dir\n",
    "\n",
    "# 실행: 밸리데이션에서 앞 5프레임만 예시로\n",
    "sids, sid_to_preds = predict_grouped_by_sid(model, val_roi, use_bottom=True, max_frames=20)\n",
    "out_dir = draw_grouped_results(val_roi, sids, sid_to_preds, out_dir=\"./preview_pred\")\n",
    "print(\"saved dir:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99cd3e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z_MAE': 2.4862086089849473,\n",
       " 'Dims_L1': 0.10567159950733185,\n",
       " 'Yaw_deg_MAE': 13.41263393770839,\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from math import atan2\n",
    "\n",
    "@torch.no_grad()\n",
    "def quick_metrics(model, dataset, max_samples=1000):\n",
    "    model.eval()\n",
    "    idxs = list(range(len(dataset)))[:max_samples]\n",
    "    err_z, err_dims, err_yaw = [], [], []\n",
    "    for i in idxs:\n",
    "        item = dataset[i]\n",
    "        img = item[\"img\"].unsqueeze(0).to(device)\n",
    "        dr, lz, yaw = model(img)\n",
    "        prior = item[\"prior\"].numpy()\n",
    "        dims_pred = (dr.squeeze(0).cpu().numpy() + prior)  # [h,w,l]\n",
    "        Z_pred = float(torch.exp(lz.squeeze()).cpu().numpy())\n",
    "        yaw_vec = yaw.squeeze(0).cpu().numpy()\n",
    "        ry_pred = float(atan2(yaw_vec[0], yaw_vec[1]))\n",
    "\n",
    "        dims_t = (item[\"dims_res\"] + item[\"prior\"]).numpy()\n",
    "        Z_t = float(torch.exp(item[\"logz\"]).numpy())\n",
    "        ry_t = float(atan2(item[\"yaw\"][0].numpy(), item[\"yaw\"][1].numpy()))\n",
    "\n",
    "        err_z.append(abs(Z_pred - Z_t))\n",
    "        err_dims.append(np.abs(dims_pred - dims_t).mean())\n",
    "        # 각도 오차 (deg) with wrap\n",
    "        d = abs((ry_pred - ry_t + np.pi)%(2*np.pi) - np.pi)\n",
    "        err_yaw.append(np.degrees(d))\n",
    "    return {\n",
    "        \"Z_MAE\": float(np.mean(err_z)),\n",
    "        \"Dims_L1\": float(np.mean(err_dims)),\n",
    "        \"Yaw_deg_MAE\": float(np.mean(err_yaw)),\n",
    "        \"N\": len(idxs)\n",
    "    }\n",
    "\n",
    "metrics = quick_metrics(model, val_roi, max_samples=1000)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "faca34be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] saved: preview_3d_bev/000021_preview_3d_bev.jpg  (sid=000021)\n"
     ]
    }
   ],
   "source": [
    "import cv2, math, numpy as np\n",
    "from pathlib import Path\n",
    "from math import atan, atan2\n",
    "\n",
    "# ===== BEV 유틸 =====\n",
    "def bev_canvas(W=600, H=600, bg=255):\n",
    "    img = np.full((H, W, 3), bg, dtype=np.uint8)\n",
    "    return img\n",
    "\n",
    "def world_to_bev(x, z, x_range=(-20,20), z_range=(0,60), W=600, H=600):\n",
    "    # x: 좌/우(카메라 좌우, +우측), z: 전방(+앞)\n",
    "    # 왼쪽->오른쪽: x_range, 아래->위: z_range\n",
    "    u = (x - x_range[0]) / (x_range[1]-x_range[0]) * (W-1)\n",
    "    v = H-1 - (z - z_range[0]) / (z_range[1]-z_range[0]) * (H-1)\n",
    "    return int(round(u)), int(round(v))\n",
    "\n",
    "def draw_rotated_bev_rect(bev, corners_xz, color, x_range=(-20,20), z_range=(0,60)):\n",
    "    pts=[]\n",
    "    for X,Z in corners_xz:\n",
    "        pts.append(world_to_bev(X, Z, x_range, z_range, bev.shape[1], bev.shape[0]))\n",
    "    pts = np.array(pts, dtype=np.int32)\n",
    "    cv2.polylines(bev, [pts], isClosed=True, color=color, thickness=2)\n",
    "\n",
    "def get_bottom_rect_xz(dims, loc, ry):\n",
    "    # compute_box_3d() 사용해서 바닥면 4코너(인덱스 0..3) 추출 (X,Z)\n",
    "    corners_3d = compute_box_3d(dims, loc, ry)  # (3,8)\n",
    "    # 카메라 뒤(z<=0)면 스킵용\n",
    "    if (corners_3d[2] <= 0).any():\n",
    "        return None\n",
    "    xs = corners_3d[0, :4]; zs = corners_3d[2, :4]\n",
    "    return list(zip(xs, zs))\n",
    "\n",
    "def estimate_half_fov_x(P2, img_w):\n",
    "    # tan(theta) = (u - cx) / fx ; 좌/우 끝에서의 절댓값 평균으로 근사\n",
    "    fx, cx = P2[0,0], P2[0,2]\n",
    "    left = abs((0 - cx)/fx)\n",
    "    right = abs((img_w - cx)/fx)\n",
    "    k = 0.5*(left + right)      # tan(half_fov)\n",
    "    theta = atan(k)\n",
    "    return theta\n",
    "\n",
    "def draw_fov_wedge(bev, P2, img_w, x_range=(-20,20), z_range=(0,60), color=(128,128,128)):\n",
    "    H, W = bev.shape[:2]\n",
    "    theta = estimate_half_fov_x(P2, img_w)  # 라디안\n",
    "    # 여러 z에 대해 x = ± z * tan(theta)\n",
    "    for sign in (-1, 1):\n",
    "        x0, z0 = 0.0, 0.0\n",
    "        x1, z1 = sign * (z_range[1]) * math.tan(theta), z_range[1]\n",
    "        p0 = world_to_bev(x0, z0, x_range, z_range, W, H)\n",
    "        p1 = world_to_bev(x1, z1, x_range, z_range, W, H)\n",
    "        cv2.line(bev, p0, p1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # 카메라 위치(원점) 점\n",
    "    p_cam = world_to_bev(0.0, 0.0, x_range, z_range, W, H)\n",
    "    cv2.circle(bev, p_cam, 4, (0,0,255), -1)\n",
    "\n",
    "# ===== 프레임 단위 예측/그리기 =====\n",
    "@torch.no_grad()\n",
    "def visualize_frame_with_bev(model, dataset, sid=None,\n",
    "                             x_range=(-20,20), z_range=(0,60),\n",
    "                             save_dir=f\"{KITTI_ROOT}/preview_combo\"):\n",
    "    model.eval()\n",
    "    save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) 어떤 프레임을 그릴지 고르기\n",
    "    if sid is None:\n",
    "        # val split에서 사용되는 프레임 id 목록\n",
    "        val_ids = sorted({s for s,_ in dataset.items})\n",
    "        sid = val_ids[0]\n",
    "    # 해당 프레임의 모든 객체 인덱스 모으기\n",
    "    idxs = [i for i,(s,_) in enumerate(dataset.items) if s == sid]\n",
    "    assert idxs, f\"{sid} 에 해당하는 ROI가 없습니다.\"\n",
    "\n",
    "    # 2) 원본 이미지 & calib 로딩\n",
    "    img_path = Path(TRAIN_DIR)/\"image_2\"/f\"{sid}.png\"\n",
    "    lbl_path = Path(TRAIN_DIR)/\"label_2\"/f\"{sid}.txt\"\n",
    "    calib = read_calib_file(str(Path(TRAIN_DIR)/\"calib\"/f\"{sid}.txt\"))\n",
    "    P2 = calib[\"P2\"]\n",
    "    img = cv2.imread(str(img_path))\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    # 3) BEV 캔버스 만들기 + FOV 쐐기선\n",
    "    bev = bev_canvas(600, 600, bg=255)\n",
    "    draw_fov_wedge(bev, P2, W, x_range, z_range, color=(180,180,180))\n",
    "\n",
    "    # 4) GT 박스(초록) 그리기 (이미지+BEV)\n",
    "    gts = [o for o in parse_label_file(str(lbl_path)) if o.cls in (\"Car\",\"Pedestrian\",\"Cyclist\")]\n",
    "    for gt in gts:\n",
    "        # 이미지(3D GT)\n",
    "        corners_3d = compute_box_3d(gt.dims, gt.loc, gt.ry)\n",
    "        if not (corners_3d[2] > 0).all():  # 카메라 뒤면 스킵\n",
    "            continue\n",
    "        pts_2d = project_to_image(corners_3d, P2)\n",
    "        img = draw_projected_box3d(img, pts_2d, (0,255,0), 2)\n",
    "\n",
    "        # BEV(바닥면)\n",
    "        rect = get_bottom_rect_xz(gt.dims, gt.loc, gt.ry)\n",
    "        if rect is not None:\n",
    "            draw_rotated_bev_rect(bev, rect, (0,160,0), x_range, z_range)\n",
    "\n",
    "    # 5) 예측 박스(주황) — ROI별로 예측 후 이미지+BEV에 그리기\n",
    "    for i in idxs:\n",
    "        item = dataset[i]\n",
    "        x = item[\"img\"].unsqueeze(0).to(device)\n",
    "        dims_res, logz, yaw = model(x)\n",
    "\n",
    "        prior = item[\"prior\"].numpy()\n",
    "        dims_pred = (dims_res.squeeze(0).cpu().numpy() + prior)   # [h,w,l]\n",
    "        Z = float(torch.exp(logz.squeeze()).cpu().numpy())\n",
    "        yaw_vec = yaw.squeeze(0).cpu().numpy()\n",
    "        ry = float(atan2(yaw_vec[0], yaw_vec[1]))\n",
    "        uv = item[\"uv\"].numpy()\n",
    "        loc = uvz_to_xyz(uv, Z, item[\"P2\"].numpy())               # [X,Y,Z]\n",
    "\n",
    "        # 이미지 투영(파랑/초록이랑 구분 위해 '주황-계열' 사용)\n",
    "        corners_3d = compute_box_3d(dims_pred, loc, ry)\n",
    "        if (corners_3d[2] <= 0).any():\n",
    "            continue\n",
    "        pts_2d = project_to_image(corners_3d, P2)\n",
    "        img = draw_projected_box3d(img, pts_2d, (0,165,255), 2)   # BGR: 주황\n",
    "\n",
    "        # BEV\n",
    "        rect = get_bottom_rect_xz(dims_pred, loc, ry)\n",
    "        if rect is not None:\n",
    "            draw_rotated_bev_rect(bev, rect, (0,165,255), x_range, z_range)\n",
    "\n",
    "    # 6) 좌우 합치기 & 저장\n",
    "    #   - 이미지 높이에 맞추어 BEV 리사이즈 후 옆으로 붙임\n",
    "    bev_resized = cv2.resize(bev, (img.shape[0], img.shape[0]))   # 정사각→이미지 높이에 맞춤\n",
    "    combo = cv2.hconcat([img, bev_resized])\n",
    "    out_path = save_dir / f\"{sid}_preview_3d_bev.jpg\"\n",
    "    cv2.imwrite(str(out_path), combo)\n",
    "    return str(out_path), sid\n",
    "\n",
    "# 실행 예시(밸리데이션 split의 첫 프레임)\n",
    "out, sid = visualize_frame_with_bev(model, val_roi, sid=\"000021\",\n",
    "                                    x_range=(-20,20), z_range=(0,60),\n",
    "                                    save_dir=f\"./preview_3d_bev\")\n",
    "print(f\"[✓] saved: {out}  (sid={sid})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d572d0",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59402a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing frames: 7518\n",
      "saved: preview_3d_bev_testing/000003_testing_combo.jpg\n"
     ]
    }
   ],
   "source": [
    "# ==== KITTI testing: 2D detector(ROI) + 우리 3D 헤드 (이미지+BEV 시각화) ====\n",
    "import cv2, math, torch, numpy as np\n",
    "from pathlib import Path\n",
    "from math import atan2\n",
    "\n",
    "TEST_DIR = \"/home/jinjinjara1022/AutonomousDriving/datasets/kitti_object/testing\"\n",
    "OUT_DIR  = \"./preview_3d_bev_testing\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 필요 유틸: (셀 1~16에서 이미 정의했다면 재사용됩니다)\n",
    "# - crop_resize, uvz_to_xyz, project_to_image, draw_projected_box3d\n",
    "# - compute_box_3d_fixed (축 교정), bev_canvas/world_to_bev/draw_rotated_bev_rect/...\n",
    "# ------------------------------------------------------------\n",
    "def read_calib_file(path:str):\n",
    "    data={}\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line: \n",
    "                continue\n",
    "            k, v = line.strip().split(\":\", 1)\n",
    "            nums = [float(x) for x in v.strip().split()] if v.strip() else []\n",
    "            if k.startswith(\"P\") and len(nums)==12:\n",
    "                data[k] = np.array(nums, np.float32).reshape(3,4)\n",
    "            elif k in (\"R0_rect\",\"R_rect\") and len(nums)==9:\n",
    "                data[k] = np.array(nums, np.float32).reshape(3,3)\n",
    "            elif k==\"Tr_velo_to_cam\" and len(nums)==12:\n",
    "                data[k] = np.array(nums, np.float32).reshape(3,4)\n",
    "    return data\n",
    "\n",
    "# 축 교정 3D 박스 (없으면 정의)\n",
    "try:\n",
    "    compute_box_3d_fixed\n",
    "except NameError:\n",
    "    def compute_box_3d_fixed(dim, loc, ry):\n",
    "        h, w, l = dim\n",
    "        x_c = [ w/2,  w/2, -w/2, -w/2,  w/2,  w/2, -w/2, -w/2]\n",
    "        y_c = [   0,    0,    0,    0,  -h,  -h,   -h,   -h]\n",
    "        z_c = [ l/2, -l/2, -l/2,  l/2,  l/2, -l/2, -l/2,  l/2]\n",
    "        C = np.vstack([x_c,y_c,z_c]).astype(np.float32)\n",
    "        c, s = math.cos(ry), math.sin(ry)\n",
    "        R = np.array([[ c,0, s],[0,1,0],[-s,0, c]], np.float32)\n",
    "        return R @ C + np.array(loc, np.float32).reshape(3,1)\n",
    "\n",
    "# BEV 유틸 (없으면 정의)\n",
    "try:\n",
    "    bev_canvas\n",
    "except NameError:\n",
    "    def bev_canvas(W=600,H=600,bg=255): return np.full((H,W,3), bg, np.uint8)\n",
    "    def world_to_bev(x,z,xr=(-20,20),zr=(0,60),W=600,H=600):\n",
    "        u = (x-xr[0])/(xr[1]-xr[0])*(W-1); v = H-1 - (z-zr[0])/(zr[1]-zr[0])*(H-1)\n",
    "        return int(round(u)), int(round(v))\n",
    "    def draw_rotated_bev_rect(bev, corners_xz, color, x_range=(-20,20), z_range=(0,60)):\n",
    "        pts=[world_to_bev(X,Z,x_range,z_range,bev.shape[1],bev.shape[0]) for X,Z in corners_xz]\n",
    "        cv2.polylines(bev, [np.int32(pts)], True, color, 2)\n",
    "    def get_bottom_rect_xz(dims, loc, ry):\n",
    "        C = compute_box_3d_fixed(dims, loc, ry)\n",
    "        if (C[2]<=0).any(): return None\n",
    "        return list(zip(C[0,:4], C[2,:4]))\n",
    "    def estimate_half_fov_x(P2, img_w):\n",
    "        fx,cx = P2[0,0], P2[0,2]\n",
    "        return math.atan(0.5*(abs((0-cx)/fx)+abs((img_w-cx)/fx)))\n",
    "    def draw_fov_wedge(bev, P2, img_w, x_range=(-20,20), z_range=(0,60), color=(180,180,180)):\n",
    "        H,W = bev.shape[:2]; th = estimate_half_fov_x(P2, img_w)\n",
    "        p0 = world_to_bev(0,0,x_range,z_range,W,H)\n",
    "        for s in (-1,1):\n",
    "            p1 = world_to_bev(s*z_range[1]*math.tan(th), z_range[1], x_range,z_range,W,H)\n",
    "            cv2.line(bev,p0,p1,color,2,cv2.LINE_AA)\n",
    "        cv2.circle(bev, p0, 4, (0,0,255), -1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2D Detector (COCO 프리트레인) → ROI (Ped/Cyc/Car만)\n",
    "# ------------------------------------------------------------\n",
    "import torchvision\n",
    "det2d = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\").to(device).eval()\n",
    "\n",
    "COCO_TO_KITTI = {1:\"Pedestrian\", 2:\"Cyclist\", 3:\"Car\", 4:\"Cyclist\", 6:\"Car\", 8:\"Car\"}\n",
    "class_to_idx = {\"Car\":0, \"Pedestrian\":1, \"Cyclist\":2}  # priors_arr 순서와 맞게 조심\n",
    "# priors_arr가 이전 셀에서 계산돼 있으면 그걸 쓰고, 없으면 기본값\n",
    "try:\n",
    "    priors_arr\n",
    "except NameError:\n",
    "    priors_arr = np.stack([\n",
    "        np.array([1.52,1.63,3.88],np.float32), # Car\n",
    "        np.array([1.73,0.60,0.80],np.float32), # Ped\n",
    "        np.array([1.73,0.60,1.76],np.float32), # Cyc\n",
    "    ],0)\n",
    "\n",
    "def run_detector_rois(img_rgb, score_thresh=0.5, max_dets=30):\n",
    "    x = torch.from_numpy(img_rgb).permute(2,0,1).float()/255.0\n",
    "    out = det2d([x.to(device)])[0]\n",
    "    boxes = out[\"boxes\"].detach().cpu().numpy()\n",
    "    labels= out[\"labels\"].detach().cpu().numpy()\n",
    "    scores= out[\"scores\"].detach().cpu().numpy()\n",
    "\n",
    "    keep=[]\n",
    "    for b,l,s in zip(boxes, labels, scores):\n",
    "        if s < score_thresh: continue\n",
    "        kcls = COCO_TO_KITTI.get(int(l))\n",
    "        if kcls not in class_to_idx: continue\n",
    "        l_,t_,r_,b_ = b.astype(int).tolist()\n",
    "        if r_-l_ < 12 or b_-t_ < 16: continue\n",
    "        keep.append({\"bbox\":[l_,t_,r_,b_], \"kcls\":kcls, \"score\":float(s)})\n",
    "        if len(keep) >= max_dets: break\n",
    "    return keep\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# testing 한 장을 \"val과 동일 스타일\"로 그리기 (GT 없음)\n",
    "# ------------------------------------------------------------\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "IMG_SIZE = 128\n",
    "PADDING_SCALE = 1.2\n",
    "\n",
    "def crop_resize(img, bbox, out_size=IMG_SIZE, scale=PADDING_SCALE):\n",
    "    h, w = img.shape[:2]\n",
    "    l,t,r,b = bbox\n",
    "    cx, cy = (l+r)/2, (t+b)/2\n",
    "    bw, bh = (r-l), (b-t)\n",
    "    s = max(bw, bh) * scale\n",
    "    x1, y1 = int(cx - s/2), int(cy - s/2)\n",
    "    x2, y2 = int(cx + s/2), int(cy + s/2)\n",
    "    # pad\n",
    "    pad_l = max(0, -x1); pad_t = max(0, -y1)\n",
    "    pad_r = max(0, x2 - w); pad_b = max(0, y2 - h)\n",
    "    if any([pad_l,pad_t,pad_r,pad_b]):\n",
    "        img = cv2.copyMakeBorder(img, pad_t, pad_b, pad_l, pad_r, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "        x1 += pad_l; x2 += pad_l; y1 += pad_t; y2 += pad_t\n",
    "    crop = img[y1:y2, x1:x2]\n",
    "    return cv2.resize(crop, (out_size, out_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def uvz_to_xyz(uv, Z, P2):\n",
    "    fx, fy = P2[0,0], P2[1,1]\n",
    "    cx, cy = P2[0,2], P2[1,2]\n",
    "    u, v = uv\n",
    "    X = (u - cx) * Z / fx\n",
    "    Y = (v - cy) * Z / fy\n",
    "    return np.array([X, Y, Z], dtype=np.float32)\n",
    "\n",
    "def project_to_image(pts_3d, P):\n",
    "    n = pts_3d.shape[1]\n",
    "    homo = np.vstack([pts_3d, np.ones((1,n), dtype=np.float32)])\n",
    "    pts_2d_homo = P @ homo\n",
    "    return pts_2d_homo[:2] / np.clip(pts_2d_homo[2:], 1e-6, None)\n",
    "\n",
    "def draw_projected_box3d(img, qs, color=(0,165,255), thickness=2):\n",
    "    qs = qs.T.astype(int)\n",
    "    edges = [(0,1),(1,2),(2,3),(3,0),(4,5),(5,6),(6,7),(7,4),(0,4),(1,5),(2,6),(3,7)]\n",
    "    for i,j in edges:\n",
    "        cv2.line(img, tuple(qs[i]), tuple(qs[j]), color, thickness)\n",
    "    return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_test_like_val(model, sid,\n",
    "                            score_thresh=0.5, max_dets=30,\n",
    "                            x_range=(-20,20), z_range=(0,60),\n",
    "                            out_dir=OUT_DIR):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    img_path  = Path(TEST_DIR)/\"image_2\"/f\"{sid}.png\"\n",
    "    calib_path= Path(TEST_DIR)/\"calib\"/f\"{sid}.txt\"\n",
    "    assert img_path.exists() and calib_path.exists(), f\"missing files for {sid}\"\n",
    "\n",
    "    img_bgr = cv2.imread(str(img_path)); img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    H,W = img_bgr.shape[:2]\n",
    "    P2  = read_calib_file(str(calib_path))[\"P2\"]\n",
    "\n",
    "    # 1) 2D detector → ROI (파랑)\n",
    "    rois = run_detector_rois(img_rgb, score_thresh=score_thresh, max_dets=max_dets)\n",
    "\n",
    "    # 2) BEV 캔버스 (전과 동일)\n",
    "    bev = bev_canvas(600,600,255); draw_fov_wedge(bev, P2, W, x_range, z_range)\n",
    "\n",
    "    # 3) 각 ROI를 우리 3D 헤드에 넣고 예측 → 이미지/BEV에 그림\n",
    "    for r in rois:\n",
    "        l,t,rgt,b = r[\"bbox\"]\n",
    "        kcls = r[\"kcls\"]\n",
    "        prior = priors_arr[class_to_idx[kcls]]\n",
    "\n",
    "        # 파랑: 2D 박스\n",
    "        cv2.rectangle(img_bgr,(l,t),(rgt,b),(255,0,0),2)\n",
    "\n",
    "        # ROI 전처리\n",
    "        crop = crop_resize(img_rgb, [l,t,rgt,b], out_size=IMG_SIZE, scale=PADDING_SCALE)\n",
    "        x = torch.from_numpy(((crop/255.0 - IMAGENET_MEAN)/IMAGENET_STD)).permute(2,0,1).unsqueeze(0).float().to(device)\n",
    "\n",
    "        # 우리 모델 추론 (dims_res, logZ, yaw(sin,cos))\n",
    "        dims_res, logz, yaw = model(x)\n",
    "        dims_pred = (dims_res.squeeze(0).cpu().numpy() + prior)      # [h,w,l]\n",
    "        Z = float(torch.exp(logz.squeeze()).cpu().numpy())\n",
    "        yv = yaw.squeeze(0).cpu().numpy()\n",
    "        ry = float(atan2(yv[0], yv[1]))\n",
    "\n",
    "        # uv: 바닥 중앙(전과 동일)\n",
    "        uv = ((l+rgt)/2.0, b)\n",
    "        loc = uvz_to_xyz(uv, Z, P2)\n",
    "\n",
    "        # 이미지에 3D 박스(주황) + BEV\n",
    "        C = compute_box_3d_fixed(dims_pred, loc, ry)\n",
    "        if (C[2] > 0).all():\n",
    "            pts = project_to_image(C, P2)\n",
    "            img_bgr = draw_projected_box3d(img_bgr, pts, (0,165,255), 2)\n",
    "            rect = get_bottom_rect_xz(dims_pred, loc, ry)\n",
    "            if rect is not None:\n",
    "                draw_rotated_bev_rect(bev, rect, (0,165,255), x_range, z_range)\n",
    "\n",
    "    # 4) 좌/우 합치기\n",
    "    bev_resized = cv2.resize(bev, (img_bgr.shape[0], img_bgr.shape[0]))\n",
    "    combo = cv2.hconcat([img_bgr, bev_resized])\n",
    "    out_path = Path(out_dir) / f\"{sid}_testing_combo.jpg\"\n",
    "    cv2.imwrite(str(out_path), combo)\n",
    "    return str(out_path)\n",
    "\n",
    "# 실행 예시 (첫 프레임)\n",
    "test_ids = sorted([p.stem for p in (Path(TEST_DIR)/\"image_2\").glob(\"*.png\")])\n",
    "print(\"testing frames:\", len(test_ids))\n",
    "p = visualize_test_like_val(model, test_ids[3], score_thresh=0.55, max_dets=25)\n",
    "print(\"saved:\", p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
