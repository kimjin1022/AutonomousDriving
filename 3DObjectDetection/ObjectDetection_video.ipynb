{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07385019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] 3D head loaded from ./models/mono3d_baseline.pt\n"
     ]
    }
   ],
   "source": [
    "# ==== KITTI Tracking → 카메라 전용 3D 시각화 영상 만들기 ====\n",
    "import os, cv2, math, torch, numpy as np\n",
    "from math import atan2\n",
    "from pathlib import Path\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------------------------ 경로 설정 ------------------------\n",
    "TRACK_ROOT = \"/home/jinjinjara1022/AutonomousDriving/datasets/KITTI_Tracking\"\n",
    "TRACK_TRAIN = f\"{TRACK_ROOT}/training\"\n",
    "TRACK_TEST  = f\"{TRACK_ROOT}/testing\"\n",
    "\n",
    "CKPT_2D = Path(\"./checkpoints/kitti2d_frcnn_all.pt\")   # 있으면 사용(우리 9클래스 헤드)\n",
    "CKPT_3D = \"./models/mono3d_baseline.pt\"                 # 반드시 존재(요청 경로)\n",
    "\n",
    "# ------------------------ 3D 유틸 ------------------------\n",
    "def compute_box_3d_fixed(dim, loc, ry):\n",
    "    h,w,l = float(dim[0]), float(dim[1]), float(dim[2])\n",
    "    x = [ w/2,  w/2, -w/2, -w/2,  w/2,  w/2, -w/2, -w/2]\n",
    "    y = [   0,    0,    0,    0,  -h,  -h,   -h,   -h]\n",
    "    z = [ l/2, -l/2, -l/2,  l/2,  l/2, -l/2, -l/2,  l/2]\n",
    "    C = np.vstack([x,y,z]).astype(np.float32)\n",
    "    c,s = math.cos(ry), math.sin(ry)\n",
    "    R = np.array([[c,0,s],[0,1,0],[-s,0,c]], np.float32)\n",
    "    return R @ C + np.array(loc, np.float32).reshape(3,1)\n",
    "\n",
    "def project_to_image(pts3d, P):\n",
    "    n = pts3d.shape[1]\n",
    "    homo = np.vstack([pts3d, np.ones((1,n), np.float32)])\n",
    "    uvw = P @ homo\n",
    "    return uvw[:2] / np.clip(uvw[2:], 1e-6, None)\n",
    "\n",
    "def uvz_to_xyz(uv, Z, P2):\n",
    "    fx, fy, cx, cy = P2[0,0], P2[1,1], P2[0,2], P2[1,2]\n",
    "    u, v = uv\n",
    "    X = (u - cx) * Z / fx\n",
    "    Y = (v - cy) * Z / fy\n",
    "    return np.array([X, Y, Z], np.float32)\n",
    "\n",
    "def draw_projected_box3d(img, qs, color=(0,165,255), thickness=2):\n",
    "    qs = qs.T.astype(int)\n",
    "    edges = [(0,1),(1,2),(2,3),(3,0),(4,5),(5,6),(6,7),(7,4),(0,4),(1,5),(2,6),(3,7)]\n",
    "    for i,j in edges: cv2.line(img, tuple(qs[i]), tuple(qs[j]), color, thickness)\n",
    "    return img\n",
    "\n",
    "# ------------------------ BEV 유틸 ------------------------\n",
    "def bev_canvas(W=600,H=600,bg=255): return np.full((H,W,3), bg, np.uint8)\n",
    "def world_to_bev(x,z,xr=(-20,20),zr=(0,60),W=600,H=600):\n",
    "    u=(x-xr[0])/(xr[1]-xr[0])*(W-1); v=H-1-(z-zr[0])/(zr[1]-zr[0])*(H-1)\n",
    "    return int(round(u)), int(round(v))\n",
    "def draw_rot_bev_rect(bev, xz, color, xr=(-20,20), zr=(0,60)):\n",
    "    pts=[world_to_bev(X,Z,xr,zr,bev.shape[1],bev.shape[0]) for X,Z in xz]\n",
    "    cv2.polylines(bev, [np.int32(pts)], True, color, 2)\n",
    "def get_bottom_rect_xz(dims, loc, ry):\n",
    "    C = compute_box_3d_fixed(dims, loc, ry)\n",
    "    if (C[2] <= 0).any(): return None\n",
    "    return list(zip(C[0,:4], C[2,:4]))\n",
    "def estimate_half_fov_x(P2, img_w):\n",
    "    fx,cx = P2[0,0], P2[0,2]\n",
    "    return math.atan(0.5*(abs((0-cx)/fx)+abs((img_w-cx)/fx)))\n",
    "def draw_fov(bev, P2, img_w, xr=(-20,20), zr=(0,60), color=(180,180,180)):\n",
    "    H,W = bev.shape[:2]; th = estimate_half_fov_x(P2, img_w)\n",
    "    p0 = world_to_bev(0,0,xr,zr,W,H)\n",
    "    for s in (-1,1):\n",
    "        p1 = world_to_bev(s*zr[1]*math.tan(th), zr[1], xr,zr,W,H)\n",
    "        cv2.line(bev,p0,p1,color,2,cv2.LINE_AA)\n",
    "    cv2.circle(bev, p0, 4, (0,0,255), -1)\n",
    "\n",
    "# ------------------------ Tracking calib (시퀀스별 P2) ------------------------\n",
    "def read_tracking_calib(seq_txt):\n",
    "    P2=None\n",
    "    with open(seq_txt,'r') as f:\n",
    "        for ln in f:\n",
    "            ln=ln.strip()\n",
    "            if ln.startswith(\"P2:\") or ln.startswith(\"P2 \"):\n",
    "                vals=[float(x) for x in ln.split()[1:]]\n",
    "                P2=np.array(vals,np.float32).reshape(3,4)\n",
    "                break\n",
    "    assert P2 is not None, f\"P2 not found in {seq_txt}\"\n",
    "    return P2\n",
    "\n",
    "# ------------------------ ROI 전처리 ------------------------\n",
    "IMAGENET_MEAN = np.array([0.485,0.456,0.406], np.float32)\n",
    "IMAGENET_STD  = np.array([0.229,0.224,0.225], np.float32)\n",
    "IMG_SIZE=128\n",
    "PAD_SCALE=1.2\n",
    "\n",
    "def crop_resize(img, bbox, out=IMG_SIZE, scale=PAD_SCALE):\n",
    "    h,w = img.shape[:2]; l,t,r,b = bbox\n",
    "    cx,cy=(l+r)/2,(t+b)/2; bw,bh=(r-l),(b-t); s=max(bw,bh)*scale\n",
    "    x1,y1=int(cx-s/2),int(cy-s/2); x2,y2=int(cx+s/2),int(cy+s/2)\n",
    "    pl=max(0,-x1); pt=max(0,-y1); pr=max(0,x2-w); pb=max(0,y2-h)\n",
    "    if pl or pt or pr or pb:\n",
    "        img=cv2.copyMakeBorder(img,pt,pb,pl,pr,cv2.BORDER_CONSTANT,value=(0,0,0))\n",
    "        x1+=pl; x2+=pl; y1+=pt; y2+=pt\n",
    "    crop=img[y1:y2, x1:x2]\n",
    "    return cv2.resize(crop,(out,out),interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# ------------------------ 2D Detector (있으면 우리 ckpt, 없으면 COCO) ------------------------\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_kitti_frcnn(num_classes):\n",
    "    m = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_feat = m.roi_heads.box_predictor.cls_score.in_features\n",
    "    m.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)  # 배경 포함\n",
    "    return m\n",
    "\n",
    "# 클래스 매핑/priors (3종)\n",
    "KITTI3 = [\"Car\",\"Pedestrian\",\"Cyclist\"]\n",
    "class_to_idx = {\"Car\":0,\"Pedestrian\":1,\"Cyclist\":2}\n",
    "priors_arr = np.stack([\n",
    "    np.array([1.52,1.63,3.88],np.float32),  # Car\n",
    "    np.array([1.73,0.60,0.80],np.float32),  # Ped\n",
    "    np.array([1.73,0.60,1.76],np.float32),  # Cyc\n",
    "],0)\n",
    "\n",
    "# 모델 구성\n",
    "if CKPT_2D.exists():\n",
    "    # 우리 9클래스 헤드 버전 (배경+KITTI8)\n",
    "    CLASSES_ALL = [\"__background__\",\"Car\",\"Van\",\"Truck\",\"Pedestrian\",\"Person_sitting\",\"Cyclist\",\"Tram\",\"Misc\"]\n",
    "    model2d = get_kitti_frcnn(num_classes=len(CLASSES_ALL)).to(device)\n",
    "    model2d.load_state_dict(torch.load(CKPT_2D, map_location=device))\n",
    "    model2d.eval()\n",
    "    USE_COCO = False\n",
    "else:\n",
    "    # COCO 프리트레인 (fallback)\n",
    "    model2d = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\").to(device).eval()\n",
    "    USE_COCO = True\n",
    "    # COCO 카테고리 → 우리 3종 매핑\n",
    "    COCO_TO_KITTI = {1:\"Pedestrian\", 2:\"Cyclist\", 3:\"Car\", 4:\"Cyclist\", 6:\"Car\", 8:\"Car\"}\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_detector_rois(img_rgb, score_thresh=0.55, max_dets=60):\n",
    "    x = torch.from_numpy(img_rgb).permute(2,0,1).float()/255.0\n",
    "    out = model2d([x.to(device)])[0]\n",
    "    boxes = out[\"boxes\"].detach().cpu().numpy().astype(int)\n",
    "    labels= out[\"labels\"].detach().cpu().numpy().astype(int)\n",
    "    scores= out[\"scores\"].detach().cpu().numpy()\n",
    "\n",
    "    rois=[]\n",
    "    H,W,_ = img_rgb.shape\n",
    "    if USE_COCO:\n",
    "        for b,l,s in zip(boxes, labels, scores):\n",
    "            if s < score_thresh: continue\n",
    "            kcls = COCO_TO_KITTI.get(int(l))\n",
    "            if kcls not in class_to_idx: continue\n",
    "            l_,t_,r_,b_ = b.tolist()\n",
    "            l_=max(0,l_); t_=max(0,t_); r_=min(W-1,r_); b_=min(H-1,b_)\n",
    "            if r_-l_ < 12 or b_-t_ < 16: continue\n",
    "            rois.append({\"bbox\":[l_,t_,r_,b_], \"cls\":kcls, \"score\":float(s)})\n",
    "            if len(rois)>=max_dets: break\n",
    "    else:\n",
    "        # 우리 헤드(9 클래스) → 3종만 필터\n",
    "        id2name = {i:n for i,n in enumerate(CLASSES_ALL)}\n",
    "        for b,l,s in zip(boxes, labels, scores):\n",
    "            if s < score_thresh: continue\n",
    "            cls = id2name.get(int(l), \"\")\n",
    "            if cls not in KITTI3: continue\n",
    "            l_,t_,r_,b_ = b.tolist()\n",
    "            l_=max(0,l_); t_=max(0,t_); r_=min(W-1,r_); b_=min(H-1,b_)\n",
    "            if r_-l_ < 12 or b_-t_ < 16: continue\n",
    "            rois.append({\"bbox\":[l_,t_,r_,b_], \"cls\":cls, \"score\":float(s)})\n",
    "            if len(rois)>=max_dets: break\n",
    "    return rois\n",
    "\n",
    "# ------------------------ 3D Head 정의 & 체크포인트 로드 ------------------------\n",
    "import torchvision.models as tvm\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mono3DHead(nn.Module):\n",
    "    def __init__(self, feat_dim=512, out_dim=6):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim,256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256,128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,out_dim)\n",
    "        )\n",
    "    def forward(self, f): return self.fc(f)\n",
    "\n",
    "class Mono3DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        m = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*(list(m.children())[:-1]))  # (B,512,1,1)\n",
    "        self.head = Mono3DHead(512, 6)\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x); f = torch.flatten(f,1)\n",
    "        o = self.head(f)\n",
    "        dims_res = o[:,0:3]\n",
    "        logz     = o[:,3:4]\n",
    "        yaw_raw  = o[:,4:6]\n",
    "        yaw = yaw_raw / (torch.linalg.norm(yaw_raw,dim=1,keepdim=True)+1e-6)\n",
    "        return dims_res, logz, yaw\n",
    "\n",
    "# 모델 만들고 ckpt 로드 (state_dict / whole model 모두 대응)\n",
    "model3d = Mono3DNet().to(device)\n",
    "_state = torch.load(CKPT_3D, map_location=device)\n",
    "try:\n",
    "    model3d.load_state_dict(_state)\n",
    "except Exception:\n",
    "    model3d = _state.to(device)\n",
    "model3d.eval()\n",
    "print(f\"[✓] 3D head loaded from {CKPT_3D}\")\n",
    "\n",
    "# ------------------------ 트래킹 시퀀스 → MP4 ------------------------\n",
    "@torch.no_grad()\n",
    "def make_kitti_tracking_video(\n",
    "    seq_id=\"0000\",                  # 시퀀스 폴더명 (4자리 문자열)\n",
    "    split=\"training\",               # \"training\" or \"testing\"\n",
    "    out_mp4=\"./track_0000.mp4\",\n",
    "    score_thresh=0.55, max_dets=60,\n",
    "    x_range=(-20,20), z_range=(0,60),\n",
    "    fps=10\n",
    "):\n",
    "    base = TRACK_TRAIN if split==\"training\" else TRACK_TEST\n",
    "    img_dir = Path(base)/\"image_02\"/seq_id\n",
    "    calib_txt = Path(base)/\"calib\"/f\"{seq_id}.txt\"   # tracking은 시퀀스별 P2\n",
    "    assert img_dir.exists(), f\"no frames at {img_dir}\"\n",
    "    assert calib_txt.exists(), f\"no calib at {calib_txt}\"\n",
    "\n",
    "    P2 = read_tracking_calib(str(calib_txt))\n",
    "    frames = sorted(img_dir.glob(\"*.png\"))\n",
    "    assert frames, f\"no images in {img_dir}\"\n",
    "\n",
    "    # 비디오 해상도 결정\n",
    "    sample = cv2.imread(str(frames[0]))\n",
    "    H,W = sample.shape[:2]\n",
    "    bev_side = H  # BEV를 이미지 높이에 맞춤\n",
    "    out_size = (W + bev_side, H)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(out_mp4, fourcc, fps, out_size)\n",
    "\n",
    "    for i,fp in enumerate(frames):\n",
    "        img_bgr = cv2.imread(str(fp))\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # BEV 초기화 + FOV\n",
    "        bev = bev_canvas(600,600,255); draw_fov(bev, P2, W, x_range, z_range)\n",
    "\n",
    "        # 2D detector → ROI (파랑)\n",
    "        rois = run_detector_rois(img_rgb, score_thresh=score_thresh, max_dets=max_dets)\n",
    "\n",
    "        for r in rois:\n",
    "            l,t,rgt,b = r[\"bbox\"]; cls = r[\"cls\"]\n",
    "            # 2D 박스\n",
    "            cv2.rectangle(img_bgr,(l,t),(rgt,b),(255,0,0),2)\n",
    "\n",
    "            # ROI 전처리 → 3D 추론\n",
    "            crop = crop_resize(img_rgb, [l,t,rgt,b], out=IMG_SIZE, scale=PAD_SCALE)\n",
    "            x = torch.from_numpy(((crop/255.0 - IMAGENET_MEAN)/IMAGENET_STD)).permute(2,0,1).unsqueeze(0).float().to(device)\n",
    "            dr, lz, yv = model3d(x)\n",
    "\n",
    "            prior = priors_arr[class_to_idx.get(cls, 0)]\n",
    "            dims  = (dr.squeeze(0).cpu().numpy() + prior)\n",
    "            Z     = float(torch.exp(lz.squeeze()).cpu().numpy())\n",
    "            ry    = float(atan2(yv.squeeze(0)[0].cpu().numpy(), yv.squeeze(0)[1].cpu().numpy()))\n",
    "\n",
    "            # (X,Y,Z) 복원: 바닥 중앙 사용\n",
    "            uv  = ((l+rgt)/2.0, b)\n",
    "            loc = uvz_to_xyz(uv, Z, P2)\n",
    "\n",
    "            # 3D 박스 (이미지+BEV)\n",
    "            C = compute_box_3d_fixed(dims, loc, ry)\n",
    "            if (C[2] > 0).all():\n",
    "                img_bgr = draw_projected_box3d(img_bgr, project_to_image(C, P2), (0,165,255), 2)\n",
    "                rect = get_bottom_rect_xz(dims, loc, ry)\n",
    "                if rect: draw_rot_bev_rect(bev, rect, (0,165,255), x_range, z_range)\n",
    "\n",
    "        bev_res = cv2.resize(bev, (H, H))\n",
    "        combo = cv2.hconcat([img_bgr, bev_res])\n",
    "        writer.write(combo)\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"{seq_id}: {i+1}/{len(frames)} frames\")\n",
    "\n",
    "    writer.release()\n",
    "    print(f\"[✓] saved video: {out_mp4}\")\n",
    "    return out_mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229467fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0003: 50/144 frames\n",
      "0003: 100/144 frames\n",
      "[✓] saved gif: ./track_0003.gif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./track_0003.gif'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "@torch.no_grad()\n",
    "def make_kitti_tracking_gif(\n",
    "    seq_id=\"0000\",                  # 시퀀스 폴더명 (4자리 문자열)\n",
    "    split=\"training\",               # \"training\" or \"testing\"\n",
    "    out_gif=\"./track_0000.gif\",\n",
    "    score_thresh=0.55, max_dets=60,\n",
    "    x_range=(-20,20), z_range=(0,60),\n",
    "    fps=10\n",
    "):\n",
    "    base = TRACK_TRAIN if split==\"training\" else TRACK_TEST\n",
    "    img_dir = Path(base)/\"image_02\"/seq_id\n",
    "    calib_txt = Path(base)/\"calib\"/f\"{seq_id}.txt\"\n",
    "    assert img_dir.exists() and calib_txt.exists()\n",
    "\n",
    "    P2 = read_tracking_calib(str(calib_txt))\n",
    "    frames = sorted(img_dir.glob(\"*.png\"))\n",
    "    assert frames, f\"no images in {img_dir}\"\n",
    "\n",
    "    # 프레임 저장용 리스트\n",
    "    gif_frames = []\n",
    "\n",
    "    for i,fp in enumerate(frames):\n",
    "        img_bgr = cv2.imread(str(fp))\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # BEV 초기화 + FOV\n",
    "        bev = bev_canvas(600,600,255); draw_fov(bev, P2, img_rgb.shape[1], x_range, z_range)\n",
    "\n",
    "        # 2D detector → ROI (파랑)\n",
    "        rois = run_detector_rois(img_rgb, score_thresh=score_thresh, max_dets=max_dets)\n",
    "\n",
    "        for r in rois:\n",
    "            l,t,rgt,b = r[\"bbox\"]; cls = r[\"cls\"]\n",
    "            #cv2.rectangle(img_bgr,(l,t),(rgt,b),(255,0,0),2)\n",
    "\n",
    "            # ROI 전처리 → 3D 추론\n",
    "            crop = crop_resize(img_rgb, [l,t,rgt,b], out=IMG_SIZE, scale=PAD_SCALE)\n",
    "            x = torch.from_numpy(((crop/255.0 - IMAGENET_MEAN)/IMAGENET_STD)).permute(2,0,1).unsqueeze(0).float().to(device)\n",
    "            dr, lz, yv = model3d(x)\n",
    "\n",
    "            prior = priors_arr[class_to_idx.get(cls, 0)]\n",
    "            dims  = (dr.squeeze(0).cpu().numpy() + prior)\n",
    "            Z     = float(torch.exp(lz.squeeze()).cpu().numpy())\n",
    "            ry    = float(atan2(yv.squeeze(0)[0].cpu().numpy(), yv.squeeze(0)[1].cpu().numpy()))\n",
    "\n",
    "            uv  = ((l+rgt)/2.0, b)\n",
    "            loc = uvz_to_xyz(uv, Z, P2)\n",
    "\n",
    "            C = compute_box_3d_fixed(dims, loc, ry)\n",
    "            if (C[2] > 0).all():\n",
    "                img_bgr = draw_projected_box3d(img_bgr, project_to_image(C, P2), (0,165,255), 2)\n",
    "                rect = get_bottom_rect_xz(dims, loc, ry)\n",
    "                if rect: draw_rot_bev_rect(bev, rect, (0,165,255), x_range, z_range)\n",
    "\n",
    "        bev_res = cv2.resize(bev, (img_bgr.shape[0], img_bgr.shape[0]))\n",
    "        combo = cv2.hconcat([img_bgr, bev_res])\n",
    "\n",
    "        # BGR → RGB 변환 후 GIF 프레임 추가\n",
    "        gif_frames.append(cv2.cvtColor(combo, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"{seq_id}: {i+1}/{len(frames)} frames\")\n",
    "\n",
    "    # GIF 저장\n",
    "    imageio.mimsave(out_gif, gif_frames, fps=fps, loop=0)\n",
    "    print(f\"[✓] saved gif: {out_gif}\")\n",
    "    return out_gif\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "make_kitti_tracking_gif(seq_id=\"0003\", split=\"training\", out_gif=\"./track_0003.gif\", fps=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
